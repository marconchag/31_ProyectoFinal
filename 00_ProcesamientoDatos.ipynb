{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerias necesiras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import locale\n",
    "import calendar\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import cdist # Para calcular distancias entre coordenadas \n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "import utm\n",
    "import openmeteo_requests\n",
    "\n",
    "import joblib\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas de los df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asignamos la ruta de directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asignamos el directorio activo\n",
    "dirActivo = os.getcwd()\n",
    "# Creamos una carpeta para que contenga a nuestro dataset\n",
    "if not os.path.isdir('datos'):os.mkdir('datos')\n",
    "path_datos = os.path.join(dirActivo+'//datos//')\n",
    "if not os.path.isdir('datos_procesados'):os.mkdir('datos_procesados')\n",
    "path_datos_procesados = os.path.join(dirActivo+'//datos_procesados//')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los csv de accidentes ubicado en la carpeta de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Definimos la funcion de carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CargarDatos(añoIni, añoFin,nombre,extension = 'csv'):\n",
    "    \"\"\"\n",
    "    Carga los datos de accidentalidad   \n",
    "    y los guarda en un archivo csv\n",
    "    añoIni: año inicial de los datos\n",
    "    añoFin: año final de los datos\n",
    "    nombre: nombre del archivo csv\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista para almacenar los DataFrames de cada archivo\n",
    "    dfs = []\n",
    "\n",
    "    # Iterar sobre los años\n",
    "    while añoIni <= añoFin:\n",
    "        # Construir el nombre del archivo para el año actual\n",
    "        nombre_archivo = f'{añoIni}_{nombre}.{extension}'\n",
    "        \n",
    "        # Intentar cargar el archivo en un DataFrame\n",
    "        try:\n",
    "            df_temporal = pd.read_csv(path_datos+nombre_archivo,sep=\";\",skipinitialspace=True,decimal=',')\n",
    "            # Agregar el DataFrame a la lista\n",
    "            dfs.append(df_temporal)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"El archivo {nombre_archivo} no existe.\")\n",
    "        \n",
    "        # Incrementar el año actual\n",
    "        añoIni += 1\n",
    "\n",
    "    # Concatenar los DataFrames en uno solo\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    #Quitamos las columnas que se crean vacias\n",
    "    df = df.iloc[:, :-2]\n",
    "\n",
    "    # Retornar el DataFrame concatenado de todos los años\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cargamos los datos de Accidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "df = CargarDatos(2019, 2024, 'Accidentalidad')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos el calendario laboral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos la información del fichero calendario\n",
    "calendario = pd.read_csv(path_datos+\"calendario.csv\", sep=\";\",skipinitialspace=True)\n",
    "#Quitamos las columnas que se crean vacias\n",
    "calendario = calendario.iloc[:, :-2]\n",
    "\n",
    "#tratamos el calendario\n",
    "reemplazos_TipoFestivo = {\n",
    "    \"de la \": \"\",\n",
    "    \"traslado \": \"\",\n",
    "    'local ciudad de Madrid': \"Local\",\n",
    "    re.compile(r'^fiesta', re.IGNORECASE): 'Festivo',\n",
    "    re.compile(r'^festivo comunidad de madrid', re.IGNORECASE): 'Festivo Comunidad',\n",
    "    re.compile(r'^Festivo nacional$', re.IGNORECASE): 'Festivo Nacional',\n",
    "    re.compile(r'^Festivo local$', re.IGNORECASE): 'Festivo Local',\n",
    "}\n",
    "\n",
    "reemplazos_Tipo = {\n",
    "    \"festivo\": \"Festivo\",\n",
    "    \"laborable\": \"Laborable\"\n",
    "}\n",
    "\n",
    "for patron, nuevo_valor in reemplazos_TipoFestivo.items():\n",
    "    calendario['Tipo de Festivo'] = calendario['Tipo de Festivo'].str.replace(patron, nuevo_valor, regex=True)\n",
    "\n",
    "calendario['laborable / festivo / domingo festivo'] = calendario['laborable / festivo / domingo festivo'].replace(reemplazos_Tipo)\n",
    "    \n",
    "festivos = calendario[calendario['laborable / festivo / domingo festivo'].str.contains('festivo|Festivo', case=False, na=False)]\n",
    "festivos\n",
    "print(festivos['Tipo de Festivo'].unique())\n",
    "print(festivos['laborable / festivo / domingo festivo'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unimos el calendario laboral al listado de accidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fecha'] = pd.to_datetime(df['fecha'], format='%d/%m/%Y')\n",
    "festivos['Dia'] = pd.to_datetime(festivos['Dia'], format='%d/%m/%Y')\n",
    "# Combinar los DataFrames en función de las columnas de fecha 'dia' y 'fecha'\n",
    "df = pd.merge(df, festivos, left_on='fecha', right_on='Dia', how='left')\n",
    "\n",
    "# Llenar los valores NaN en la columna 'festivo' con 'laborable'\n",
    "df['laborable / festivo / domingo festivo'].fillna('Laborable', inplace=True)\n",
    "\n",
    "# Asignar 'fin de semana' si el día es sábado o domingo\n",
    "df.loc[df['fecha'].dt.dayofweek.isin([5, 6]), 'laborable / festivo / domingo festivo'] = 'Fin de semana'\n",
    "\n",
    "\n",
    "# Ahora df contiene todos los días con los valores filtrados asignados\n",
    "df\n",
    "df.drop(columns=['Dia','Dia_semana'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ponemos el día de la semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función a la columna 'fecha' para obtener el nombre del día de la semana\n",
    "df['dia_semana'] = df['fecha'].dt.strftime('%A')\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "df['dia_semana'].unique()\n",
    "\n",
    "df['dia_semana'] = df['dia_semana'].apply(lambda x: x.encode('latin1').decode('utf-8'))\n",
    "df['dia_semana'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisamos los valores nulos por columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de valores nulos en cada columna\n",
    "valores_nulos_por_columna = df.isnull().sum()\n",
    "\n",
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "porcentaje_nulos_por_columna = (valores_nulos_por_columna / len(df)) * 100\n",
    "\n",
    "# Crear un DataFrame para mostrar los resultados\n",
    "resultados_nulos = pd.DataFrame({'Valores Nulos': valores_nulos_por_columna, 'Porcentaje': porcentaje_nulos_por_columna})\n",
    "\n",
    "# Filtrar solo las columnas que tienen valores nulos\n",
    "resultados_nulos = resultados_nulos[resultados_nulos['Valores Nulos'] > 0]\n",
    "\n",
    "print(\"Columnas con valores nulos:\")\n",
    "print(resultados_nulos.sort_values('Porcentaje', ascending=False))\n",
    "\n",
    "#Generamos graficos valores nulos\n",
    "fig = px.imshow(df.isnull(), \n",
    "                labels=dict(color=\"Valores Nulos\"),\n",
    "                title=\"Valores Nulos\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estado meteorológico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haciendo uso de la API de Open-Meteo tenemos opción de consultar datos históricos y datos futuros\n",
    "no detecta las distintas coordenadas por lo tanto unificamos datos de consulta a Madrid para imputar datos vacíos\n",
    "\n",
    "* Latitud : 40.4165\n",
    "* Longitud : -3.70256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificar los valores\n",
    "mapeo_valores = {\n",
    "    'Lluvia débil': 'Lluvia',\n",
    "    'LLuvia intensa': 'Lluvia',\n",
    "    'Lluvia intensa': 'Lluvia'\n",
    "    # Aquí puedes agregar más mapeos si es necesario\n",
    "}\n",
    "df['estado_meteorológico'] = df['estado_meteorológico'].replace(mapeo_valores)\n",
    "\n",
    "estado_meteorológico_anterior = df['estado_meteorológico'].unique()\n",
    "estado_meteorológico_anterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_description = {\n",
    "    0: 'Despejado',\n",
    "    1: \"Despejado\",\n",
    "    2: \"Nublado\",\n",
    "    3: \"Nublado\",\n",
    "    45: \"Niebla\",\n",
    "    48: \"Niebla con depósito de escarcha\",\n",
    "    51: \"Lluvia\", #Lluvia débil\n",
    "    53: \"Lluvia\", #Lluvia débil\n",
    "    55: \"Lluvia\", #Lluvia débil\n",
    "    56: \"Llovizna helada ligera\",\n",
    "    57: \"Llovizna helada de intensidad densa\",\n",
    "    61: \"Lluvia\",  #Lluvia débil\n",
    "    63: \"Lluvia\",  #Lluvia moderada\n",
    "    65: \"Lluvia\",  #Lluvia intensa\n",
    "    66: \"Lluvia helada ligera\",\n",
    "    67: \"Lluvia helada de intensidad fuerte\",\n",
    "    71: \"Nevando\",\n",
    "    73: \"Nevando\",\n",
    "    75: \"Nevando\",\n",
    "    77: \"Nevando\",\n",
    "    80: \"Lluvia\",   #Lluvia intensa\n",
    "    81: \"Lluvia\",   #Lluvia intensa\n",
    "    82: \"Lluvia\",   #Lluvia intensa\n",
    "    85: \"Nevando\",\n",
    "    86: \"Nevando\",\n",
    "    95: \"Tormenta\",\n",
    "    96: \"Granizando\",\n",
    "    99: \"Granizando\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos las Fecha inicial y Fecha final para la consulta\n",
    "fecha_ini = df['fecha'].min().strftime('%Y-%m-%d')\n",
    "fecha_fin = df['fecha'].max().strftime('%Y-%m-%d')\n",
    "print(f\"Fecha Inicial: {fecha_ini} fecha Final: {fecha_fin}\")\n",
    "\n",
    "# Instanciamos el clientes\n",
    "openmeteo = openmeteo_requests.Client()\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "\t\"latitude\": 40.4165,\n",
    "\t\"longitude\": -3.70256,\n",
    "\t\"start_date\": fecha_ini,\n",
    "\t\"end_date\": fecha_fin,\n",
    "\t\"hourly\": \"weather_code\",\n",
    "\t\"daily\": \"weather_code\",\n",
    "\t\"timezone\": \"auto\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "# print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "# print(f\"Elevation {response.Elevation()} m asl\")\n",
    "# print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "# print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_weather_code = hourly.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "# print(f'{hourly_dataframe=}')\n",
    "\n",
    "# Process daily data. The order of variables needs to be the same as requested.\n",
    "daily = response.Daily()\n",
    "daily_weather_code = daily.Variables(0).ValuesAsNumpy()\n",
    "\n",
    "daily_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = daily.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "daily_data[\"weather_code\"] = daily_weather_code\n",
    "\n",
    "daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "# print(f'{daily_dataframe=}')\n",
    "\n",
    "# Agregar descripciones a los DataFrames\n",
    "hourly_dataframe['weather_description'] = hourly_dataframe['weather_code'].map(weather_description)\n",
    "daily_dataframe['weather_description'] = daily_dataframe['weather_code'].map(weather_description)\n",
    "\n",
    "# Convertir la columna 'fecha_hora' a tipo datetime\n",
    "hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])\n",
    "\n",
    "# Crear columnas separadas para fecha y hora\n",
    "hourly_dataframe['fecha'] = hourly_dataframe['date'].dt.date\n",
    "hourly_dataframe['hora'] = hourly_dataframe['date'].dt.time\n",
    "\n",
    "# Eliminar la columna 'fecha_hora' si ya no la necesitas\n",
    "hourly_dataframe.drop(columns=['date'], inplace=True)\n",
    "hourly_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_dataframe['weather_description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formateomops la columna Hora\n",
    "df['hora'] = pd.to_datetime(df['hora'], format='%H:%M:%S', errors='coerce')\n",
    "df['hora'] = df['hora'].dt.time\n",
    "\n",
    "# Construir un diccionario que mapee la combinación de fecha y hora a weather_description\n",
    "weather_dict = {}\n",
    "for index, row in hourly_dataframe.iterrows():\n",
    "    fecha_hora_redondeada = row['fecha'].strftime('%Y-%m-%d'), row['hora'].hour\n",
    "    weather_dict[fecha_hora_redondeada] = row['weather_description']\n",
    "\n",
    "# Actualizar el DataFrame original utilizando el diccionario\n",
    "condicion = (df['estado_meteorológico'].isnull()) | (df['estado_meteorológico'] == 'Se desconoce')\n",
    "indices_a_actualizar = df.index[condicion]\n",
    "df.loc[indices_a_actualizar, 'estado_meteorológico'] = df.loc[indices_a_actualizar].apply(lambda row: weather_dict.get((row['fecha'].strftime('%Y-%m-%d'), row['hora'].hour), None), axis=1)\n",
    "\n",
    "\n",
    "#Mostramos los valores de Antes y Despues \n",
    "estado_meteorológico_nuevo = df['estado_meteorológico'].unique()\n",
    "\n",
    "print(f'{estado_meteorológico_anterior}\\n{estado_meteorológico_nuevo}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lesividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anterior =df[['lesividad','cod_lesividad']].value_counts()\n",
    "\n",
    "# Quitamos los espacios en blanco para despues poder asignar un valor a los NaN\n",
    "df['lesividad'] = df['lesividad'].str.strip()\n",
    "\n",
    "# Completamos los valores NaN con 'Sin asistencia sanitaria' y '14' respectivamente\n",
    "df.loc[(df['lesividad'].isna()) & (df['cod_lesividad'].isna()), \n",
    "        ['lesividad', 'cod_lesividad']] = ['Sin asistencia sanitaria', 14]\n",
    "\n",
    "\n",
    "# Cambiamos los valores de 'Se Desconoce' y '77' a 'Sin asistencia sanitaria' y '14' respectivamente\n",
    "df.loc[(df['lesividad'] == 'Se desconoce') & (df['cod_lesividad'] == 77),\n",
    "        ['lesividad', 'cod_lesividad']] = ['Sin asistencia sanitaria', 14]\n",
    "\n",
    "# Crear un diccionario de clasificación de lesividad\n",
    "clasificacion_lesividad = {\n",
    "    1.0: 'Leve',\n",
    "    2.0: 'Grave',\n",
    "    3.0: 'Grave',\n",
    "    4.0: 'Fallecido',\n",
    "    5.0: 'Leve',\n",
    "    6.0: 'Leve',\n",
    "    7.0: 'Leve',\n",
    "    14.0: 'Muy leve'\n",
    "    \n",
    "}\n",
    "\n",
    "# Aplicar la clasificación de lesividad y crear una nueva columna\n",
    "df['tipo_lesividad'] = df['cod_lesividad'].apply(lambda x: clasificacion_lesividad.get(x, 'No definido'))\n",
    "\n",
    "posterior = df[['lesividad','cod_lesividad']].value_counts()\n",
    "\n",
    "print(f'{anterior=}\\n\\n{posterior=}')\n",
    "\n",
    "# Mostrar el DataFrame con la nueva columna\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupamos los datos para ver las clasificaciones con la cantidad y el %\n",
    "lesividad = df.groupby(['lesividad','cod_lesividad']).size().reset_index(name='cantidad')\n",
    "suma_total = lesividad['cantidad'].sum()\n",
    "lesividad['porcentaje'] = (lesividad['cantidad'] / suma_total) * 100\n",
    "lesividad.sort_values('cod_lesividad', ascending=False, inplace=True)\n",
    "lesividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupamos los datos para ver las clasificaciones con la cantidad y el %\n",
    "lesividad = df.groupby(['tipo_lesividad']).size().reset_index(name='cantidad')\n",
    "suma_total = lesividad['cantidad'].sum()\n",
    "lesividad['porcentaje'] = (lesividad['cantidad'] / suma_total) * 100\n",
    "lesividad.sort_values('cantidad', ascending=False, inplace=True)\n",
    "lesividad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alcohol y Drogas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar NaN en 'positiva_alcohol' con 'N' y convertir a valores numéricos\n",
    "df['positiva_alcohol'] = df['positiva_alcohol'].fillna('N').replace({'N': 0, 'S': 1})\n",
    " \n",
    "# Reemplazar NaN por 0 en la columna 'positiva_droga'\n",
    "df['positiva_droga'].fillna(0, inplace=True)\n",
    " \n",
    "# Filtrar filas donde se detectó alcohol y drogas simultáneamente\n",
    "ambos_detectados = df[(df['positiva_alcohol'] == 1) & (df['positiva_droga'] == 1)]\n",
    " \n",
    "# Contar el número de casos donde se detectó alcohol y drogas simultáneamente\n",
    "num_ambos_detectados = len(ambos_detectados)\n",
    " \n",
    "print(f\"Número de casos donde se detectó tanto alcohol como drogas: {num_ambos_detectados}\")\n",
    " \n",
    "# Calcular el total de casos donde se detectó alcohol\n",
    "total_alcohol = df['positiva_alcohol'].sum()\n",
    " \n",
    "# Calcular el total de casos donde se detectaron drogas\n",
    "total_drogas = df['positiva_droga'].sum()\n",
    " \n",
    "# Calcular el total de casos donde se detectó tanto alcohol como drogas\n",
    "total_ambos = len(ambos_detectados)\n",
    " \n",
    "# Calcular el porcentaje de casos donde se detectó tanto alcohol como drogas con respecto al total de casos\n",
    "porcentaje_ambos = (total_ambos / len(df)) * 100\n",
    " \n",
    "print(f\"Porcentaje de casos donde se detectó tanto alcohol como drogas: {porcentaje_ambos:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tipo_persona'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisamos los expedientes en los que aparece el tipo persona vacio\n",
    "\n",
    "# Crear una lista de valores únicos de 'num_expediente' para las filas donde 'tipo_persona' es nulo\n",
    "expedientes = df[df['tipo_persona'].isnull()]['num_expediente'].unique()\n",
    "\n",
    "# Crear una máscara booleana que indica si cada valor en la columna 'num_expediente' está en la lista de valores a filtrar\n",
    "mascara_filtro = df['num_expediente'].isin(expedientes)\n",
    "\n",
    "# Aplicar la máscara booleana al DataFrame para filtrar las filas\n",
    "df_filtrado = df[mascara_filtro]\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasajero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como en todos los expientes tenemos conductores vamos a comletar los datos vacios con pasajero\n",
    "df.loc[df['tipo_persona'].isnull(), 'tipo_persona'] = 'Pasajero'\n",
    "\n",
    "# Crear una máscara booleana que indica si cada valor en la columna 'num_expediente' está en la lista de valores a filtrar\n",
    "mascara_filtro = df['num_expediente'].isin(expedientes)\n",
    "\n",
    "# Aplicar la máscara booleana al DataFrame para filtrar las filas\n",
    "df_filtrado = df[mascara_filtro]\n",
    "df_filtrado\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo Accidente\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conductores = df[(df['tipo_persona'] == 'Conductor') & (df['tipo_accidente'].notna())]\n",
    "\n",
    "# Calcular el número de vehículos implicados por expediente\n",
    "implicados = conductores.groupby(['distrito', 'num_expediente', 'tipo_accidente'])['tipo_vehiculo'].count().reset_index()\n",
    "implicados = implicados.rename(columns={'tipo_vehiculo': 'implicados'})\n",
    "\n",
    "# Calcular la moda del tipo de accidente por distrito y número de vehículos implicados\n",
    "moda_por_grupo = implicados.groupby(['distrito', 'implicados'])['tipo_accidente'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "moda_por_grupo\n",
    "\n",
    "# Filtrar los registros vacíos en la columna 'tipo_accidente'\n",
    "vacios = df[df['tipo_accidente'].isnull()]\n",
    "\n",
    "# Agrupar por número de expediente y calcular el número de vehículos implicados para cada expediente\n",
    "vacios = vacios.groupby(['distrito', 'num_expediente']).size().reset_index(name='implicados')\n",
    "\n",
    "\n",
    "for index, row in vacios.iterrows():\n",
    "\n",
    "    #Buscamos el tipo de accidente por Distrito y Número implicados\n",
    "\n",
    "    tipo_accidente = moda_por_grupo[(moda_por_grupo['distrito'] == row['distrito']) & (moda_por_grupo['implicados'] == row['implicados'])]['tipo_accidente']\n",
    "\n",
    "        # Verificar si se encontró un tipo de accidente en moda_por_grupo\n",
    "    if not tipo_accidente.empty:        \n",
    "        # Asignar el tipo de accidente en df para el número de expediente correspondiente\n",
    "        df.loc[(df['num_expediente'] == row['num_expediente']) & (df['tipo_accidente'].isnull()), 'tipo_accidente'] = tipo_accidente.iloc[0]\n",
    "\n",
    "# Unificamos datos \n",
    "#Unificamos los valores de la columna tipo_accidente\n",
    "reemplazos_TipoAccidente = {\n",
    "    'Colisión múltiple':'Colisión',\n",
    "    'Colisión fronto-lateral':'Colisión',\n",
    "    'Colisión lateral':'Colisión',\n",
    "    'Colisión frontal':'Colisión',\n",
    "    'Alcance':'Colisión',\n",
    "    'Atropello a persona':'Atropello',\n",
    "    'Atropello a animal':'Atropello',\n",
    "    'Despeñamiento':'Caída',\n",
    "    'Choque contra obstáculo fijo': 'Colisión',\n",
    "    'Solo salida de la vía': 'Otro',\n",
    "    'Otro':'Otro'\n",
    "}\n",
    "\n",
    "df['tipo_accidente'] = df['tipo_accidente'].replace(reemplazos_TipoAccidente)\n",
    "df['tipo_accidente'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisamos valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de valores nulos en cada columna\n",
    "valores_nulos_por_columna = df.isnull().sum()\n",
    "\n",
    "# Calcular el porcentaje de valores nulos en cada columna\n",
    "porcentaje_nulos_por_columna = (valores_nulos_por_columna / len(df)) * 100\n",
    "\n",
    "# Crear un DataFrame para mostrar los resultados\n",
    "resultados_nulos = pd.DataFrame({'Valores Nulos': valores_nulos_por_columna, 'Porcentaje': porcentaje_nulos_por_columna})\n",
    "\n",
    "# Filtrar solo las columnas que tienen valores nulos\n",
    "resultados_nulos = resultados_nulos[resultados_nulos['Valores Nulos'] > 0]\n",
    "\n",
    "print(\"Columnas con valores nulos:\")\n",
    "print(resultados_nulos.sort_values('Porcentaje', ascending=False))\n",
    "\n",
    "#Generamos graficos valores nulos\n",
    "fig = px.imshow(df.isnull(), \n",
    "                labels=dict(color=\"Valores Nulos\"),\n",
    "                title=\"Valores Nulos\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos Latitud y Longitud desde UTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hacemos la conversión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para convertir coordenadas UTM a longitud y latitud\n",
    "def utm_to_latlon(x_utm, y_utm):\n",
    "    if isinstance(x_utm, str) and isinstance(y_utm, str):\n",
    "        try:\n",
    "            # Reemplazar la coma por un punto y luego convertir a flotante\n",
    "            x_utm = float(x_utm.replace(',', '.'))\n",
    "            y_utm = float(y_utm.replace(',', '.'))\n",
    "\n",
    "            if 100000 <= x_utm <= 999999 and 0 <= y_utm <= 10000000:\n",
    "                lat, lon = utm.to_latlon(x_utm, y_utm, zone_number=30, northern=True)\n",
    "                return lat, lon\n",
    "            else :\n",
    "                return None, None\n",
    "\n",
    "        except ValueError:\n",
    "            # Si hay un error al convertir, devolver None para latitud y longitud\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "# Aplicar la función a las columnas 'coordenada_x_utm' y 'coordenada_y_utm' para crear nuevas columnas 'latitud' y 'longitud'\n",
    "df['latitud'], df['longitud'] = zip(*df.apply(lambda row: utm_to_latlon(row['coordenada_x_utm'], row['coordenada_y_utm']), axis=1))\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame con las nuevas columnas de coordenadas geográficas\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Completamos con la moda por distrito si alguna de ellas no se ha podido completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores no válidos en longitud y latitud\n",
    "df_valid = df.dropna(subset=['longitud', 'latitud'])\n",
    " \n",
    "# Calcular la moda de longitud y latitud para cada distrito que tenga datos disponibles\n",
    "mode_longitud = df_valid.groupby('distrito')['longitud'].apply(lambda x: mode(x)[0] if len(x) > 0 else np.nan).to_dict()\n",
    "mode_latitud = df_valid.groupby('distrito')['latitud'].apply(lambda x: mode(x)[0] if len(x) > 0 else np.nan).to_dict()\n",
    " \n",
    "# Llenar los valores faltantes con la moda\n",
    "df['longitud'] = df['longitud'].fillna(df['distrito'].map(mode_longitud))\n",
    "df['latitud'] = df['latitud'].fillna(df['distrito'].map(mode_latitud))\n",
    " \n",
    "# Mostrar las primeras filas del DataFrame con las nuevas coordenadas geográficas llenadas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo Vehiculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los valores anteriores\n",
    "anterior = df.tipo_vehiculo.unique()\n",
    "\n",
    "# Rellenamos los NaN como sin especificar\n",
    "df['tipo_vehiculo'].fillna('Sin especificar', inplace=True)\n",
    "\n",
    "# Generamos el diccionario para unificar valores\n",
    "reemplazos_TipoVehiculo = {\n",
    "    'Motocicleta > 125cc':'Motocicleta',\n",
    "    'Ciclomotor':'Motocicleta',\n",
    "    'Motocicleta hasta 125cc':'Motocicleta',\n",
    "    'Todo terreno':'Turismo',\n",
    "    'Camión rígido':'Vehiculo Pesado',\n",
    "    'Maquinaria de obras':'Vehiculo Pesado',\n",
    "    'Tractocamión' :'Vehiculo Pesado',\n",
    "    'Cuadriciclo no ligero':'Motocicleta',\n",
    "    'Vehículo articulado':'Vehiculo Pesado',\n",
    "    'Autobús articulado' :'Autobús',\n",
    "    'Otros vehículos con motor':'Otros',\n",
    "    'Patinete' :'Otros',\n",
    "    'Ciclo':'Motocicleta',\n",
    "    'Cuadriciclo ligero':'Motocicleta',\n",
    "    'VMU eléctrico':'Turismo',\n",
    "    'Semiremolque':'Semiremolque',\n",
    "    'Microbús <= 17 plazas':'Autobús',\n",
    "    'Autobus EMT':'Autobús',\n",
    "    'Otros vehículos sin motor':'Otros',\n",
    "    'Bicicleta EPAC (pedaleo asistido)':'Bicicleta',\n",
    "    'Bicicleta EPAC (pedaleo asistido)':'Bicicleta',\n",
    "    'Moto de tres ruedas > 125cc':'Motocicleta',\n",
    "    'Ambulancia SAMUR':'Ambulancia',\n",
    "    'Moto de tres ruedas hasta 125cc':'Motocicleta',\n",
    "    'Ciclomotor de dos ruedas L1e-B':'Motocicleta',\n",
    "    'Maquinaria agrícola' :'Vehiculo Pesado',\n",
    "    'Autobús articulado EMT':'Autobús',\n",
    "    'Autobús EMT':'Autobús',\n",
    "    'Motocicleta de motor L1e-A':'Motocicleta',\n",
    "    'Ciclomotor de tres ruedas':'Motocicleta',\n",
    "    'Ciclo de motor L1e-A':'Motocicleta',\n",
    "    'Patinete no eléctrico':'Otros',\n",
    "    'Motocicleta de dos ruedas L1e-B':'Motocicleta',\n",
    "    'Motocicleta de tres ruedas':'Motocicleta',\n",
    "    'Otros no eléctrico':'Otros',\n",
    "    'Bicicleta EPAC (pedaleo asistido)':'Bicicleta',\n",
    "    'Furgoneta':'Turismo',\n",
    "    'Caravana':'Turismo',\n",
    "    'Autocaravana':'Turismo',\n",
    "    'Semiremolque':'Turismo',\n",
    "    'Remolque':'Turismo',\n",
    "    'Camión de bomberos':'Emergencias',\n",
    "    'Ambulancia':'Emergencias',\n",
    "    'Tranvía':\"Tren/metro\",\n",
    "    'Sin especificar':'Otros'\n",
    "}\n",
    "\n",
    "# Unificamos los vehiculos segun la definicion en el diccionario\n",
    "df['tipo_vehiculo'] = df['tipo_vehiculo'].replace(reemplazos_TipoVehiculo)\n",
    "\n",
    "# Guardamos los datos despues de la reclasificación\n",
    "posterior = df.tipo_vehiculo.unique()\n",
    "\n",
    "#Mostramos los valores antes y despues de la unificación\n",
    "print(f'{anterior=} \\n\\n {posterior=}')\n",
    "\n",
    "del anterior , posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Localización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Función para eliminar el número de calle después de la coma\n",
    "# def eliminar_numero_despues_de_coma(localizacion):\n",
    "#     if ',' in localizacion:\n",
    "#         return localizacion.split(',')[0].strip()  # Obtener la parte antes de la coma y eliminar espacios en blanco adicionales\n",
    "#     else:\n",
    "#         return localizacion  # Si no hay coma, devolver la localización sin cambios\n",
    " \n",
    "# # Aplicar la función a la columna 'localizacion' para eliminar el número de calle después de la coma\n",
    "# df['localizacion'] = df['localizacion'].apply(eliminar_numero_despues_de_coma)\n",
    " \n",
    "# # Mostrar las primeras filas del DataFrame con la columna 'localizacion' modificada\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tipo de via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Función para identificar el tipo de vía\n",
    "# def identificar_tipo_via(localizacion):\n",
    "#     # Expresión regular para buscar los patrones específicos\n",
    "#     patron_cruce = r'\\/'\n",
    "#     patron_m30 = r'M-30|M -30'\n",
    "#     patron_a1= r'A-1'\n",
    "#     patron_a2= r'A-2'\n",
    "#     patron_a3= r'A-3'\n",
    "#     patron_a4= r'A-4'\n",
    "#     patron_a5= r'A-5'\n",
    "#     patron_a6= r'A-6'\n",
    "#     patron_m40= r'M-40|M -40'\n",
    "#     patron_plaza = r'plaza|PLAZA|PLZ.|Atocha'\n",
    "#     patron_calle = r'call|cañada|cñada|custa|km|bulev|ronda|alcala|c/|calle|CALL.|GRAN VIA|SANTA ENGRACIA|C '\n",
    "#     patron_paseo = r'paseo'\n",
    "#     patron_avenida = r'avenida|avda|avd|av'\n",
    "#     patron_carretera = r'crta|carretera|CTRA.|CARRETERA|CRA.' #no se por que me estaá dando error CARRETERA y CTRA:\n",
    "#     patron_glorieta = r'GTA.|GLORIETA|glorieta|glta'\n",
    "#     patron_camino = r'cmno|camino|CAMINO|TRVA.'\n",
    "#     patron_tunel = r'túnel|TUNEL|TUNEL.|tunel |subterraneo'\n",
    "#     patron_autovia = r'M-301|AUTOV.'\n",
    "#     patro_aeropuerto = r'Aeropuerto|Aerop'\n",
    "#     patron_autopista = r'A-42'\n",
    " \n",
    "#     # Buscar el patrón de cruce, autovía, calle, paseo, avenida.... y si existe el patron devolver lo que se le pida\n",
    "#     match_cruce = re.search(patron_cruce, localizacion)\n",
    "#     if match_cruce:\n",
    "#         return 'Cruce'\n",
    "#     match_m30 = re.search(patron_m30, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_m30:\n",
    "#         return 'M-30'  \n",
    "#     match_a1 = re.search(patron_a1, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a1:\n",
    "#         return 'A-1'\n",
    "#     match_a2 = re.search(patron_a2, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a2:\n",
    "#         return 'A-2'\n",
    "#     match_a3 = re.search(patron_a3, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a3:\n",
    "#         return 'A-3'\n",
    "#     match_a4 = re.search(patron_a4, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a4:\n",
    "#         return 'A-4'\n",
    "#     match_a5 = re.search(patron_a5, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a5:\n",
    "#         return 'A-5'\n",
    "#     match_a6 = re.search(patron_a6, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_a6:\n",
    "#         return 'A-6'\n",
    "#     match_m40 = re.search(patron_m40, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_m40:\n",
    "#         return 'M-40'   \n",
    "#     match_plaza = re.search(patron_plaza, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_plaza:\n",
    "#         return 'Plaza'  \n",
    "#     match_calle = re.search(patron_calle, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_calle:\n",
    "#         return 'Calle'\n",
    "#     match_paseo = re.search(patron_paseo, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_paseo:\n",
    "#         return 'Paseo'\n",
    "#     match_avenida = re.search(patron_avenida, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_avenida:\n",
    "#         return 'Avenida'\n",
    "#     match_carretera = re.search(patron_carretera, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_carretera:\n",
    "#         return 'Carretera'\n",
    "#     match_glorieta = re.search(patron_glorieta, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_glorieta:\n",
    "#         return 'Glorieta'  \n",
    "#     match_camino = re.search(patron_camino, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_camino:\n",
    "#         return 'Camino'\n",
    "#     match_tunel = re.search(patron_tunel, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_tunel:\n",
    "#         return 'Túnel'\n",
    "#     match_autovia = re.search(patron_autovia, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_autovia:\n",
    "#         return 'Autovía'\n",
    "#     match_aeropuerto = re.search(patro_aeropuerto, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_aeropuerto:\n",
    "#         return 'Aeropuerto'\n",
    "#     match_autopista = re.search(patron_autopista, localizacion, flags=re.IGNORECASE)\n",
    "#     if match_autopista:\n",
    "#         return 'Autopista'\n",
    "    \n",
    "#     return 'Calle'  # Si no se encuentra ningún patrón, devolver 'desconocido'\n",
    "\n",
    "# # Aplicar la función a la columna 'localizacion' para crear la nueva columna 'tipo_via'\n",
    "# df['tipo_via'] = df['localizacion'].apply(identificar_tipo_via)\n",
    "\n",
    "# # Mostrar las primeras filas del DataFrame con la nueva columna 'tipo_via'\n",
    "# df.head()\n",
    "\n",
    "# df['tipo_via'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ponemos el nombre de la calle y el distrito por los puntos mas cercanos en el callejero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callejero = pd.read_csv(f'{path_datos}callejeroMadrid.csv',encoding='iso-8859-1',delimiter=';',decimal=',')\n",
    "distritos = pd.read_excel(f'{path_datos}distritos.xlsx')\n",
    "\n",
    "# accidentes = pd.read_csv(f'{path_datos_procesados}accidentes_procesados.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Función para convertir coordenadas geográficas en formato de grados, minutos y segundos a decimal\n",
    "# Función para convertir coordenadas geográficas en formato de grados, minutos y segundos a decimal\n",
    "def dms_to_decimal(coordenada):\n",
    "    # Expresión regular para extraer los componentes de la coordenada\n",
    "    match = re.match(r\"([-+]?\\d+)°(\\d+)'([\\d.]+)''\\s*([NSWE])\", coordenada)\n",
    "    if match:\n",
    "        grados, minutos, segundos, direccion = match.groups()\n",
    "        decimal = float(grados) + float(minutos)/60 + float(segundos)/3600\n",
    "        if direccion in ['S', 'W']:\n",
    "            decimal = -decimal\n",
    "        return decimal\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "callejero['LATITUD'] = callejero['LATITUD'].apply(lambda x: dms_to_decimal(x) if isinstance(x, str) else x)\n",
    "callejero['LONGITUD'] = callejero['LONGITUD'].apply(lambda x: dms_to_decimal(x) if isinstance(x, str) else x)\n",
    "callejero.rename(columns={'LATITUD': 'latitud', 'LONGITUD': 'longitud'}, inplace=True)\n",
    "callejero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la distancia euclidiana entre dos puntos\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "# Inicializar el diccionario para almacenar información por expediente\n",
    "info_por_expediente = {}\n",
    "\n",
    "# Agrupar los accidentes por número de expediente\n",
    "for num_expediente, grupo in df.groupby('num_expediente'):\n",
    "    latitudes_accidentes = grupo['latitud'].values.reshape(-1, 1)\n",
    "    longitudes_accidentes = grupo['longitud'].values.reshape(-1, 1)\n",
    "    coordenadas_accidentes = np.hstack((latitudes_accidentes, longitudes_accidentes))\n",
    "    \n",
    "    # Calcular las distancias a todas las coordenadas del callejero\n",
    "    distancias = cdist(coordenadas_accidentes, callejero[['latitud', 'longitud']], metric='euclidean')\n",
    "    \n",
    "    # Encontrar el índice de la calle más cercana para cada accidente del expediente\n",
    "    indices_cercanos = distancias.argmin(axis=1)\n",
    "    \n",
    "    # Obtener el nombre y la clase de la calle más cercana para cada accidente\n",
    "    calles_cercanas = callejero.loc[indices_cercanos, ['VIA_NOMBRE_ACENTOS', 'VIA_CLASE', 'DISTRITO']]\n",
    "    \n",
    "    # Tomar la primera calle más cercana para representar el expediente\n",
    "    calle_mas_cercana = calles_cercanas.iloc[0]\n",
    "    \n",
    "    # Guardar la información en el diccionario por número de expediente\n",
    "    info_por_expediente[num_expediente] = calle_mas_cercana\n",
    "\n",
    "# Mapear las calles más cercanas a cada expediente en el DataFrame de accidentes\n",
    "df['callejero_calle'] = df['num_expediente'].map(lambda x: info_por_expediente[x]['VIA_NOMBRE_ACENTOS'])\n",
    "df['callejero_tipo_via'] = df['num_expediente'].map(lambda x: info_por_expediente[x]['VIA_CLASE'])\n",
    "df['callejero_distrito'] = df['num_expediente'].map(lambda x: info_por_expediente[x]['DISTRITO'])\n",
    "\n",
    "\n",
    "# Renombrar las columnas para que coincidan\n",
    "distritos.rename(columns={'COD_DIS': 'callejero_distrito', 'DISTRI_MT': 'distrito_mt'}, inplace=True)\n",
    "\n",
    "# Fusionar los DataFrames por la columna 'cod_distrito' y agregar solo la columna 'distrito_mt'\n",
    "df = pd.merge(df, distritos[['callejero_distrito', 'distrito_mt']], on='callejero_distrito', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asignamos tramo horario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos la columna hora en hora\n",
    "df['hora'] = pd.to_datetime(df['hora'], format='%H:%M:%S', errors='coerce')\n",
    "df['hora'] = df['hora'].dt.time\n",
    "\n",
    "# Definimos la función para asignar la franja horaria\n",
    "def asignar_franja_horaria(hora):\n",
    "    if hora.hour >= 0 and hora.hour < 7:\n",
    "        tramo = 'Madrugada'\n",
    "    elif hora.hour >= 7 and hora.hour < 14:\n",
    "        tramo = 'Mañana'\n",
    "    elif hora.hour >= 14 and hora.hour < 21:\n",
    "        tramo ='Tarde'\n",
    "    else:\n",
    "        tramo ='Noche'\n",
    "    return tramo \n",
    "# Creamos una columna nueva para la franja horaria\n",
    "df['franja_horaria'] = df['hora'].apply(asignar_franja_horaria).apply(pd.Series)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asignamos tramos de edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame auxiliar para facilitar el cálculo de la moda\n",
    "edades_df = df.copy()\n",
    "\n",
    "# Reemplazamos 'Desconocido' con NaN para que no se considere en el cálculo de la moda\n",
    "edades_df['rango_edad'].replace('Desconocido', np.nan, inplace=True)\n",
    "\n",
    "# Agrupamos por tipo de vehículo y tipo de persona y calculamos la moda de rango_edad\n",
    "moda_por_grupo = edades_df.groupby(['tipo_vehiculo', 'tipo_persona'])['rango_edad'].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).reset_index()\n",
    "\n",
    "# Iteramos sobre el DataFrame original para asignar las edades desconocidas\n",
    "for index, row in df.iterrows():\n",
    "    if row['rango_edad'] == 'Desconocido':\n",
    "        tipo_vehiculo = row['tipo_vehiculo']\n",
    "        tipo_persona = row['tipo_persona']\n",
    "        moda = moda_por_grupo[(moda_por_grupo['tipo_vehiculo'] == tipo_vehiculo) & (moda_por_grupo['tipo_persona'] == tipo_persona)]['rango_edad'].values\n",
    "        if moda:\n",
    "            df.at[index, 'rango_edad'] = moda[0]\n",
    "\n",
    "# Verificamos el conteo de valores después de la corrección\n",
    "df['rango_edad'].value_counts()\n",
    "\n",
    "# Mapeamos los rangos de edad a los grupos definidos\n",
    "edad_reemplazo = {\n",
    "    'De 45 a 49 años': '40-49',\n",
    "    'De 30 a 34 años': '30-39',\n",
    "    'De 40 a 44 años': '40-49',\n",
    "    'De 65 a 69 años': '65-69',\n",
    "    'Más de 74 años': '+74',\n",
    "    'De 21 a 24 años': '18-29',\n",
    "    'De 35 a 39 años': '30-39',\n",
    "    'De 50 a 54 años': '50-59',\n",
    "    'De 60 a 64 años': '60-64',\n",
    "    'De 55 a 59 años': '50-59',\n",
    "    'De 15 a 17 años': '0-17',\n",
    "    'De 18 a 20 años': '18-29',\n",
    "    'De 25 a 29 años': '18-29',\n",
    "    'De 70 a 74 años': '70-74',\n",
    "    'De 6 a 9 años': '0-17',\n",
    "    'Menor de 5 años': '0-17',\n",
    "    'De 10 a 14 años': '0-17'\n",
    "}\n",
    "\n",
    "# Creamos una nueva columna que contenga las edades agrupadas\n",
    "df['grupo_edad'] = df['rango_edad'].replace(edad_reemplazo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sexo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 'Desconocido' con NaN para que no se considere en el cálculo de la moda\n",
    "df['sexo'].replace('Desconocido', np.nan, inplace=True)\n",
    "\n",
    "# Define las agrupaciones iniciales\n",
    "agrupaciones = ['tipo_vehiculo', 'tipo_persona', 'tipo_accidente', 'grupo_edad']\n",
    "\n",
    "# Define una lista para almacenar las agrupaciones que se probarán\n",
    "agrupaciones_a_probar = agrupaciones.copy()\n",
    "\n",
    "# Itera sobre las agrupaciones a probar\n",
    "while len(agrupaciones_a_probar) > 0:\n",
    "    # Calcula la moda de sexo por las agrupaciones actuales\n",
    "    moda_sexo = df.groupby(agrupaciones_a_probar)['sexo'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    # Imputa los valores de moda en las filas donde el sexo es NaN\n",
    "    df['sexo'] = df.apply(lambda row: moda_sexo.get(tuple(row[agrupaciones_a_probar]), row['sexo']) if pd.isna(row['sexo']) else row['sexo'], axis=1)\n",
    "\n",
    "    # Verifica si todavía hay valores NaN en el sexo\n",
    "    if df['sexo'].isna().sum() == 0:\n",
    "        # Si no hay valores NaN, la imputación se completó, termina el bucle\n",
    "        print(f\"Imputación completada con las agrupaciones: {agrupaciones_a_probar}\")\n",
    "        break\n",
    "    else:\n",
    "        # Si todavía hay valores NaN, elimina una columna de las agrupaciones y vuelve a intentarlo\n",
    "        print(f\"Imputación incompleta con las agrupaciones: {agrupaciones_a_probar} valores vacíos {df['sexo'].isna().sum()}\")\n",
    "        agrupaciones_a_probar.pop()\n",
    "\n",
    "# Verifica si la imputación fue completamente exitosa\n",
    "if df['sexo'].isna().sum() > 0:\n",
    "    print(f\"Imputación final incompleta. Quedan {df['sexo'].isna().sum()} valores vacíos.\")\n",
    "else:\n",
    "    print(\"Imputación final completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por número de expediente y sumar el total de conductores, pasajeros y peatones\n",
    "total_por_expediente = df.groupby('num_expediente')['tipo_persona'].value_counts().unstack(fill_value=0)\n",
    "total_por_expediente = total_por_expediente.rename(columns={1: 'Conductor', 2: 'Pasajero', 3: 'Peatón'})\n",
    "total_por_expediente['Total'] = total_por_expediente['Conductor'] + total_por_expediente['Peatón']\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(total_por_expediente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_datos_procesados+'accidentesCalles_procesados.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_datos_procesados+'accidentesCalles_procesados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.drop(columns=['localizacion','numero','cod_distrito','distrito','rango_edad','cod_lesividad','lesividad','coordenada_x_utm',\n",
    "                    'coordenada_y_utm','Tipo de Festivo', 'Festividad','callejero_distrito'], inplace=True)\n",
    "    df.columns\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar las columnas del DataFrame df\n",
    "df = df.rename(columns={\n",
    "    'num_expediente': 'Expediente',\n",
    "    'fecha': 'Fecha',\n",
    "    'dia_semana':'Día semana',\n",
    "    'laborable / festivo / domingo festivo':'Tipo día',\n",
    "    'hora': 'Hora',\n",
    "    'distrito_mt':'Distrito',\n",
    "    'callejero_tipo_via':'Tipo de vía',\n",
    "    'callejero_calle':'Calle',\n",
    "    'latitud':'Latitud',\n",
    "    'longitud':'Longitud',\n",
    "    'tipo_accidente': 'Tipo accidente',\n",
    "    'estado_meteorológico' : 'Estado meteorológico',\n",
    "    'tipo_vehiculo':'Tipo vehiculo',\n",
    "    'tipo_persona':'Implicado',\n",
    "    'grupo_edad':'Edad',\n",
    "    'sexo':'Sexo',\n",
    "    'tipo_lesividad' : 'Lesividad',\n",
    "    'positiva_alcohol':'Positivo alcohol',\n",
    "    'positiva_droga':'Positivo droga',\n",
    "    'franja_horaria':'Tramo horario'\n",
    "    \n",
    "})\n",
    "\n",
    "column_order = ['Expediente', 'Fecha','Día semana','Tipo día','Hora','Tramo horario','Distrito','Tipo de vía','Calle','Latitud','Longitud','Tipo accidente',\n",
    "                'Estado meteorológico','Tipo vehiculo','Implicado','Sexo','Edad','Lesividad','Positivo alcohol','Positivo droga']\n",
    "df = df.reindex(columns=column_order)\n",
    "\n",
    "# Ponemos Distrito / Tipo de Vía y Calle como nombre propio\n",
    "df[['Distrito', 'Tipo de vía', 'Calle']] = df[['Distrito', 'Tipo de vía', 'Calle']].apply(lambda x: x.str.title())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_datos_procesados+'accidentes_procesados.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamos los datos procesados por expediente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tablas de frecuencia cruzada para cada variable categórica\n",
    "agrupado = df.groupby(['Expediente','Fecha','Día semana','Tipo día','Hora','Tramo horario','Distrito','Tipo de vía','Calle','Latitud','Longitud','Tipo accidente','Estado meteorológico']).size().reset_index(name='Implicados')\n",
    "sexo_cruzado = pd.crosstab(index=df['Expediente'], columns=df['Sexo'])\n",
    "implicado_cruzado = pd.crosstab(index=df['Expediente'], columns=df['Implicado'])\n",
    "edad_cruzada = pd.crosstab(index=df['Expediente'], columns=df['Edad'])\n",
    "lesividad_cruzada = pd.crosstab(index=df['Expediente'], columns=df['Lesividad'])\n",
    "tipo_vehiculo_cruzado = pd.crosstab(index=df['Expediente'], columns=df['Tipo vehiculo'])\n",
    "\n",
    "accidentes = agrupado.merge(sexo_cruzado, on='Expediente', how='left').merge(implicado_cruzado, on='Expediente', how='left').merge(edad_cruzada, on='Expediente', how='left').merge(lesividad_cruzada, on='Expediente', how='left').merge(tipo_vehiculo_cruzado, on='Expediente', how='left')\n",
    "\n",
    "accidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidentes.to_csv(path_datos_procesados+'accidentes_procesados_agrupados.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamos los datos por Distrito y asignamos la latitud y la longitud por distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distritos_madrid = df['Distrito'].unique()\n",
    "distritos_madrid\n",
    "\n",
    "\n",
    "# Crear una lista de diccionarios para almacenar los datos\n",
    "datos_distritos = []\n",
    "\n",
    "# Inicializar el geocodificador de Nominatim\n",
    "geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "\n",
    "# Obtener las coordenadas para cada distrito\n",
    "for distrito in distritos_madrid:\n",
    "    location = geolocator.geocode(f\"{distrito}, Madrid, Spain\")\n",
    "    if location:\n",
    "        datos_distritos.append({\n",
    "            \"Distrito\": distrito,\n",
    "            \"Latitud\": location.latitude,\n",
    "            \"Longitud\": location.longitude\n",
    "        })\n",
    "    else:\n",
    "        print(f\"No se pudo encontrar la ubicación para {distrito}\")\n",
    "\n",
    "# Convertir la lista de diccionarios en un DataFrame\n",
    "df_distritos = pd.DataFrame(datos_distritos)\n",
    "\n",
    "# Mostrar el DataFrame con las coordenadas de los distritos\n",
    "print(df_distritos)\n",
    "\n",
    "accidentesDistrito = df['Distrito'].value_counts().reset_index()\n",
    "df_final = pd.merge(df_distritos, accidentesDistrito, on='Distrito', how='left')\n",
    "df_final\n",
    "df_final.to_csv(f'{path_datos_procesados}accidentes_distritos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ponemos fecha y hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que las columnas de fecha y hora estén en formato adecuado\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "df['Hora'] = pd.to_timedelta(df['Hora'])\n",
    "\n",
    "# Combina la información de fecha y hora en una sola columna de fecha y hora\n",
    "df['Fecha_Hora'] = df['Fecha'] + df['Hora']\n",
    "\n",
    "# Muestra el DataFrame con la nueva columna de fecha y hora\n",
    "print(df[['Fecha', 'Hora', 'Fecha_Hora']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparamos los datos para ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_datos_procesados+'accidentes_procesados.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación ordinal\n",
    "Asigna a cada categoría un valor numérico único en función de su orden o importancia. Esto es útil cuando las categorías tienen un orden natural, como por ejemplo, \"bajo\", \"medio\" y \"alto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codificamos el tramo de edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar la categoría '+74' a '74+'\n",
    "dfML['Edad'] = dfML['Edad'].replace('+74', '74+')\n",
    "\n",
    "# Realizar Ordinal Encoding en la columna 'Edad'\n",
    "dfML['Edad_codificada'] = dfML['Edad'].astype('category').cat.codes\n",
    "\n",
    "# Agrupar por 'Edad' y 'Edad_codificada' y contar el número de implicados en cada grupo\n",
    "agrupado = dfML.groupby(['Edad', 'Edad_codificada']).size().reset_index(name='Implicados')\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(agrupado)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Codificamos Lesividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el orden de las categorías de Lesividad en función de su importancia (invertido)\n",
    "categorias_lesividad = [\n",
    "    'Muy leve',\n",
    "    'Leve',\n",
    "    'Grave',\n",
    "    'Fallecido'\n",
    "]\n",
    "\n",
    "# Crear una instancia de pd.Categorical con el orden especificado\n",
    "lesividad_categorica = pd.Categorical(dfML['Lesividad'], categories=categorias_lesividad, ordered=True)\n",
    "\n",
    "# Asignar los códigos resultantes a una nueva columna\n",
    "dfML['Lesividad_codificada'] = lesividad_categorica.codes\n",
    "\n",
    "# Agrupar por 'Lesividad' y 'Lesividad_codificada' y contar el número de implicados en cada grupo\n",
    "agrupado = dfML.groupby(['Lesividad', 'Lesividad_codificada']).size().reset_index(name='Implicados')\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(agrupado)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación sinusoidal\n",
    "\n",
    "Esta técnica utiliza funciones sinusoidales para codificar variables cíclicas, como días de la semana, horas del día o meses del año. Se basa en la idea de que los datos temporales tienen una naturaleza periódica y pueden ser representados por funciones sinusoidales. La codificación sinusoidal asigna valores que oscilan entre -1 y 1 para capturar la fase de la función sinusoidal en relación con el tiempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Día semana / Mes Año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que la columna de fecha esté en formato datetime\n",
    "dfML['Fecha'] = pd.to_datetime(dfML['Fecha'])\n",
    "\n",
    "# Obtener el día de la semana como un número entero (de 0 a 6 donde 0 es lunes y 6 es domingo)\n",
    "dia_semana = dfML['Fecha'].dt.dayofweek\n",
    "mes = dfML['Fecha'].dt.month\n",
    "\n",
    "# Calcular la codificación sinusoidal de los días de la semana y de los meses\n",
    "# Utilizamos la función sinusoidal para capturar la naturaleza cíclica de los días de la semana\n",
    "dfML['Dia_semana_codificado'] = np.sin(2 * np.pi * dia_semana / 7)\n",
    "# Utilizamos la función sinusoidal para capturar la naturaleza cíclica de los meses del año\n",
    "dfML['Mes_codificado'] = np.sin(2 * np.pi * mes / 12)\n",
    "dfML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Horas del día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la hora del día como un número entero (de 0 a 23)\n",
    "dfML['Hora'] = pd.to_datetime(dfML['Hora'])\n",
    "hora_dia = dfML['Hora'].dt.hour\n",
    "\n",
    "# Calcular la codificación sinusoidal de las horas del día\n",
    "# Utilizamos la función sinusoidal para capturar la naturaleza cíclica de las horas del día\n",
    "dfML['Hora_codificada'] = np.sin(2 * np.pi * hora_dia / 24)\n",
    "dfML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encodign\n",
    "Reemplaza cada categoría con la media (o alguna otra estadística) del valor objetivo correspondiente a esa categoría. Esta técnica puede ser útil cuando hay una relación entre la variable categórica y la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calle / Tipo de vía / Distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el codificador de frecuencia\n",
    "frequency_encoder_calle = ce.CountEncoder(cols=['Calle'])\n",
    "frequency_encoder_via = ce.CountEncoder(cols=['Tipo de vía'])\n",
    "frequency_encoder_distrito = ce.CountEncoder(cols=['Distrito'])\n",
    "\n",
    "# Aplicar la codificación de frecuencia a las columnas 'Calle', 'Tipo de vía' y 'Distrito'\n",
    "dfML['Calle_encoded_frequency'] = frequency_encoder_calle.fit_transform(dfML['Calle']).values.ravel()\n",
    "dfML['TipoVia_encoded_frequency'] = frequency_encoder_via.fit_transform(dfML['Tipo de vía']).values.ravel()\n",
    "dfML['Distrito_encoded_frequency'] = frequency_encoder_distrito.fit_transform(dfML['Distrito']).values.ravel()\n",
    "\n",
    "# Guardar el codificador de frecuencia en un archivo Joblib\n",
    "# joblib.dump(frequency_encoder, 'frequency_encoder.joblib')\n",
    "\n",
    "# Inicializar el codificador de objetivo\n",
    "target_encoder = ce.TargetEncoder(cols=['Calle'])\n",
    "\n",
    "# Aplicar el target encoding a la columna 'Calle' utilizando la columna 'Lesividad_codificada' como objetivo\n",
    "dfML['Calle_encoded_target'] = target_encoder.fit_transform(dfML['Calle'], dfML['Lesividad_codificada']).values.ravel()\n",
    "\n",
    "# Guardar el codificador de objetivo en un archivo Joblib\n",
    "# joblib.dump(target_encoder, 'target_encoder.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de impacto\n",
    "Similar al target encoding, pero utiliza un enfoque bayesiano para estimar las probabilidades de la variable objetivo condicionadas a cada categoría.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tipo de Accidente / Implicado / Estado Meteorológico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializar el codificador de objetivo promedio para las variables 'Tipo accidente' e 'Implicado'\n",
    "impact_encoder_tipo_accidente = ce.TargetEncoder(cols=['Tipo accidente'])\n",
    "impact_encoder_implicado = ce.TargetEncoder(cols=['Implicado'])\n",
    "impact_encoder_meteorológico = ce.TargetEncoder(cols=['Estado meteorológico'])\n",
    "\n",
    "# Aplicar el codificador de objetivo promedio a las variables 'Tipo accidente' e 'Implicado'\n",
    "dfML['Tipo_accidente_impact_encoded'] = impact_encoder_tipo_accidente.fit_transform(dfML['Tipo accidente'], dfML['Lesividad_codificada'])\n",
    "dfML['Implicado_impact_encoded'] = impact_encoder_implicado.fit_transform(dfML['Implicado'], dfML['Lesividad_codificada'])\n",
    "dfML['Estado_meteorológico_impact_encoded'] = impact_encoder_meteorológico.fit_transform(dfML['Estado meteorológico'], dfML['Lesividad_codificada'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoder\n",
    "Convierte cada valor único de la columna de edad en una nueva columna binaria (0 o 1), indicando la presencia o ausencia de ese valor en cada fila. Esto puede ser útil si no hay un orden inherente en las categorías y no quieres introducir un supuesto de ordinalidad en tus datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Aplicar one-hot encoding a la columna 'sexo'\n",
    "dfML = pd.get_dummies(dfML, columns=['Sexo'])\n",
    "\n",
    "# Obtener las nuevas columnas generadas por get_dummies\n",
    "nuevas_columnas = dfML.columns[dfML.columns.str.startswith('Sexo_')]\n",
    "\n",
    "# Convertir True a 1 y False a 0 solo en las nuevas columnas\n",
    "dfML[nuevas_columnas] = dfML[nuevas_columnas].astype(int)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "dfML\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
